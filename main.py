# main.py
import os
import sys
import pandas as pd
import numpy as np

# Add the current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def check_imports():
    """Check if all required packages are installed"""
    required_packages = [
        'numpy', 'pandas', 'sklearn', 'torch', 'matplotlib', 
        'seaborn', 'plotly', 'statsmodels', 'xgboost', 'scipy'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            if package == 'sklearn':
                __import__('sklearn')
            else:
                __import__(package)
            print(f"‚úì {package} imported successfully")
        except ImportError:
            missing_packages.append(package)
            print(f"‚úó {package} not found")
    
    if missing_packages:
        print(f"\nMissing packages: {missing_packages}")
        print("Please install them using: pip install " + " ".join(missing_packages))
        return False
    return True

def create_sample_data():
    """Create sample data files for testing in both CSV and Excel formats"""
    print("\nCreating sample data files...")
    
    # Create sample engagement data
    time_points = 1000
    timestamps = np.arange(0, time_points, 0.1)  # 10 Hz data
    
    # Sample ENG data
    eng_data = pd.DataFrame({
        'timestamp': timestamps,
        'engagement': np.sin(timestamps * 0.1) + 0.5 * np.random.normal(size=len(timestamps))
    })
    
    # Sample EEG data (multiple channels)
    eeg_data = pd.DataFrame({
        'timestamp': timestamps,
        'eeg_delta': np.sin(timestamps * 0.2) + 0.3 * np.random.normal(size=len(timestamps)),
        'eeg_theta': np.cos(timestamps * 0.15) + 0.3 * np.random.normal(size=len(timestamps)),
        'eeg_alpha': np.sin(timestamps * 0.25) + 0.3 * np.random.normal(size=len(timestamps)),
        'eeg_beta': np.sin(timestamps * 0.3) + 0.3 * np.random.normal(size=len(timestamps)),
        'eeg_gamma': np.cos(timestamps * 0.35) + 0.3 * np.random.normal(size=len(timestamps))
    })
    
    # Sample GSR data
    gsr_data = pd.DataFrame({
        'timestamp': timestamps,
        'gsr_value': np.abs(np.sin(timestamps * 0.05)) + 0.2 * np.random.normal(size=len(timestamps))
    })
    
    # Sample Eye data
    eye_data = pd.DataFrame({
        'timestamp': timestamps,
        'pupil_size': 3 + 0.5 * np.sin(timestamps * 0.1) + 0.1 * np.random.normal(size=len(timestamps)),
        'fixation_duration': np.random.exponential(0.5, len(timestamps)),
        'saccade_amplitude': np.random.normal(5, 1, len(timestamps))
    })
    
    # Sample IVT data
    ivt_data = pd.DataFrame({
        'timestamp': timestamps,
        'blink_rate': np.random.poisson(0.1, len(timestamps)),
        'fixation_count': np.random.poisson(2, len(timestamps))
    })
    
    # Save sample data in both CSV and Excel formats for session 1
    os.makedirs('data/raw/session1', exist_ok=True)
    
    # Save as CSV
    eng_data.to_csv('data/raw/session1/1_ENG.csv', index=False)
    eeg_data.to_csv('data/raw/session1/1_EEG.csv', index=False)
    gsr_data.to_csv('data/raw/session1/1_GSR.csv', index=False)
    eye_data.to_csv('data/raw/session1/1_EYE.csv', index=False)
    ivt_data.to_csv('data/raw/session1/1_IVT.csv', index=False)
    
    # Save as Excel
    eng_data.to_excel('data/raw/session1/1_ENG.xlsx', index=False)
    eeg_data.to_excel('data/raw/session1/1_EEG.xlsx', index=False)
    gsr_data.to_excel('data/raw/session1/1_GSR.xlsx', index=False)
    eye_data.to_excel('data/raw/session1/1_EYE.xlsx', index=False)
    ivt_data.to_excel('data/raw/session1/1_IVT.xlsx', index=False)
    
    # Also create session 2 data
    os.makedirs('data/raw/session2', exist_ok=True)
    
    eng_data_2 = pd.DataFrame({
        'timestamp': timestamps,
        'engagement': np.cos(timestamps * 0.1) + 0.5 * np.random.normal(size=len(timestamps))
    })
    
    # Save session 2 data
    eng_data_2.to_csv('data/raw/session2/2_ENG.csv', index=False)
    eeg_data.to_csv('data/raw/session2/2_EEG.csv', index=False)
    gsr_data.to_csv('data/raw/session2/2_GSR.csv', index=False)
    eye_data.to_csv('data/raw/session2/2_EYE.csv', index=False)
    ivt_data.to_csv('data/raw/session2/2_IVT.csv', index=False)
    
    eng_data_2.to_excel('data/raw/session2/2_ENG.xlsx', index=False)
    eeg_data.to_excel('data/raw/session2/2_EEG.xlsx', index=False)
    gsr_data.to_excel('data/raw/session2/2_GSR.xlsx', index=False)
    eye_data.to_excel('data/raw/session2/2_EYE.xlsx', index=False)
    ivt_data.to_excel('data/raw/session2/2_IVT.xlsx', index=False)
    
    print("Sample data created in data/raw/session1/ and data/raw/session2/")
    print("Files created in both CSV and Excel formats")

def check_data_exists(session_num):
    """Check if data files exist for a session"""
    session_path = f'data/raw/session{session_num}'
    
    if not os.path.exists(session_path):
        return False
    
    required_files = ['ENG', 'EEG', 'GSR', 'EYE', 'IVT']
    files_found = []
    
    for file_type in required_files:
        csv_file = f'{session_path}/{session_num}_{file_type}.csv'
        excel_file = f'{session_path}/{session_num}_{file_type}.xlsx'
        
        if os.path.exists(csv_file) or os.path.exists(excel_file):
            files_found.append(file_type)
    
    print(f"Session {session_num}: Found {len(files_found)}/{len(required_files)} data files")
    return len(files_found) > 0

def create_simple_features(df):
    """Simple feature engineering: adds a rolling mean of engagement as a new feature"""
    df = df.copy()
    if 'engagement' in df.columns:
        df['engagement_roll'] = df['engagement'].rolling(window=5, min_periods=1).mean()
    else:
        df['engagement_roll'] = 0
    return df

# Ultra-simple features
def ultra_simple_features(processed_df):
    """Ultra-simple feature engineering that will definitely work"""
    print("   Using ultra-simple feature engineering...")
    
    features_list = []
    targets = []
    
    window_size = 10
    step_size = 2
    
    for i in range(0, len(processed_df) - window_size, step_size):
        try:
            window = processed_df.iloc[i:i + window_size]
            target = processed_df.iloc[i + window_size]
            
            if 'engagement' not in target:
                continue
                
            window_features = {}
            
            numeric_cols = processed_df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col not in ['timestamp', 'engagement']][:10]
            
            for col in numeric_cols:
                if col in window.columns:
                    col_data = window[col].values
                    window_features[f'{col}_mean'] = float(np.mean(col_data))
            
            if window_features:
                features_list.append(window_features)
                targets.append(float(target['engagement']))
                
        except:
            continue
    
    if not features_list:
        print("   Creating trivial features as last resort...")
        features_list = [{'dummy_feature': 1} for _ in range(min(10, len(processed_df)))]
        targets = [0.5] * len(features_list)
    
    features_df = pd.DataFrame(features_list)
    features_df['engagement'] = targets
    
    return features_df

def run_complete_pipeline():
    """Run the complete engagement prediction pipeline"""
    print("\nStarting Engagement Prediction Pipeline...")
    
    try:
        from process.preprocess import DataPreprocessor
        from process.feature_engineering import FeatureEngineer
        from process.baseline_model import BaselineModels
        from process.lstm_model import LSTMTrainer
        from process.transformer_model import TransformerTrainer
        from process.analysis import ResultAnalyzer
        
        print("‚úì All modules imported successfully")
        
        preprocessor = DataPreprocessor()
        feature_engineer = FeatureEngineer(window_size=10, step_size=2)
        baseline_models = BaselineModels()
        lstm_trainer = LSTMTrainer(sequence_length=20)
        transformer_trainer = TransformerTrainer(sequence_length=20)
        analyzer = ResultAnalyzer()
        
        for session_num in [1, 2]:
            print(f"\n{'='*50}")
            print(f"Processing Session {session_num}")
            print(f"{'='*50}")
            
            if not check_data_exists(session_num):
                print(f"‚ö† No data found for session {session_num}, skipping...")
                continue
            
            try:
                print("1. Preprocessing data...")
                try:
                    processed_df, scaler = preprocessor.preprocess_session(session_num)
                    print(f"   ‚úì Processed data shape: {processed_df.shape}")
                    
                    features_df, selector = feature_engineer.engineer_features(processed_df)
                    
                    if 'engagement' not in processed_df.columns:
                        print("   ‚ö† No engagement data found. Creating synthetic engagement for demonstration.")
                        if len(processed_df) > 0:
                            normalized_time = (processed_df['timestamp'] - processed_df['timestamp'].min()) / (processed_df['timestamp'].max() - processed_df['timestamp'].min())
                            processed_df['engagement'] = 0.5 + 0.3 * np.sin(normalized_time * 4 * np.pi)
                        else:
                            processed_df['engagement'] = 0.5
                            
                except Exception as e:
                    print(f"   ‚ùå Preprocessing failed: {e}")
                    print("   Skipping to next session...")
                
                if 'processed_df' not in locals():
                    print("   ‚ö† No processed data available, skipping session")
                
                print("2. Engineering features...")
                features_df = None

                try:
                    features_df, selector = feature_engineer.engineer_features(processed_df)
                    print(f"   ‚úì Main features shape: {features_df.shape}")
                except Exception as e:
                    print(f"   ‚ö† Main feature engineering failed: {e}")
                    
                    try:
                        features_df = ultra_simple_features(processed_df)
                        print(f"   ‚úì Ultra-simple features shape: {features_df.shape}")
                    except Exception as e2:
                        print(f"   ‚ùå Ultra-simple also failed: {e2}")
                        print("   Skipping to next session...")
                        continue

                if features_df is None or len(features_df) == 0:
                    print("   ‚ùå No features created, skipping session...")
                    continue

                try:
                    features_df.to_csv(f'data/features/session{session_num}_features.csv', index=False)
                    print(f"   ‚úì Features saved to data/features/session{session_num}_features.csv")
                except Exception as e:
                    print(f"   ‚ö† Could not save features: {e}")
            
                print("3. Training baseline models...")
                results, predictions = baseline_models.train_all_models(features_df)
                
                print("\n   Baseline Model Results:")
                for model_name, metrics in results.items():
                    print(f"     {model_name}:")
                    print(f"       MAE: {metrics['test_mae']:.4f}")
                    print(f"       RMSE: {metrics['test_rmse']:.4f}")
                    print(f"       R¬≤: {metrics['test_r2']:.4f}")
                
                # üî• Updated: Train LSTM with 100 epochs, batch size 32
                print("4. Training LSTM model...")
                try:
                    train_losses, test_losses = lstm_trainer.train_model(
                        features_df, epochs=100, batch_size=32
                    )
                    
                    lstm_metrics, lstm_predictions, attention, y_test = lstm_trainer.evaluate_model(features_df)
                    print(f"   ‚úì LSTM Results: MAE: {lstm_metrics['mae']:.4f}, RMSE: {lstm_metrics['rmse']:.4f}, R¬≤: {lstm_metrics['r2']:.4f}")
                except Exception as e:
                    print(f"   ‚ö† LSTM training skipped: {e}")
                
                # üî• Updated: Train Transformer with 100 epochs, batch size 32
                print("5. Training Transformer model...")
                try:
                    transformer_losses, transformer_test_losses = transformer_trainer.train_model(
                        features_df, epochs=100, batch_size=32
                    )
                    
                    transformer_metrics, transformer_predictions, y_test_transformer = transformer_trainer.evaluate_model(features_df)
                    print(f"   ‚úì Transformer Results: MAE: {transformer_metrics['mae']:.4f}, RMSE: {transformer_metrics['rmse']:.4f}, R¬≤: {transformer_metrics['r2']:.4f}")
                except Exception as e:
                    print(f"   ‚ö† Transformer training skipped: {e}")
                
                print("6. Generating analysis...")
                importance_df = baseline_models.get_feature_importance(features_df)
                importance_df.to_csv(f'results/session{session_num}_feature_importance.csv', index=False)
                
                import matplotlib.pyplot as plt
                plt.figure(figsize=(10, 6))
                top_features = importance_df.head(10)
                plt.barh(top_features['feature'], top_features['importance'])
                plt.xlabel('Importance')
                plt.title(f'Top 10 Features for Engagement Prediction - Session {session_num}')
                plt.tight_layout()
                plt.savefig(f'results/session{session_num}_feature_importance.png', dpi=300, bbox_inches='tight')
                plt.close()
                
                print("   ‚úì Analysis completed and saved to results/")
                
                report = f"""
                ENGAGEMENT PREDICTION PIPELINE - EXECUTION REPORT
                =================================================
                
                Session: {session_num}
                Execution Time: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
                
                DATA PROCESSING:
                - Original data processed successfully
                - Features engineered: {features_df.shape[1] - 1} features
                - Final dataset shape: {features_df.shape}
                
                MODEL PERFORMANCE:
                """
                
                for model_name, metrics in results.items():
                    report += f"- {model_name}: MAE={metrics['test_mae']:.4f}, R¬≤={metrics['test_r2']:.4f}\n"
                
                report += f"""
                NEXT STEPS:
                - Add your real data files to data/raw/session{session_num}/
                - Adjust window sizes and hyperparameters
                - Run on complete dataset for final results
                """
                
                with open(f'results/session{session_num}_execution_report.txt', 'w') as f:
                    f.write(report)
                
                print(f"   ‚úì Session {session_num} processing completed successfully!")
                
            except Exception as e:
                print(f"   ‚ùå Error processing session {session_num}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print("\nüéâ Pipeline execution completed!")
        
    except Exception as e:
        print(f"‚ùå Error during pipeline execution: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("Engagement Level Prediction System")
    print("=" * 50)
    
    if not check_imports():
        print("\nPlease install missing packages and try again.")
        sys.exit(1)
    
    os.makedirs('data/raw/session1', exist_ok=True)
    os.makedirs('data/raw/session2', exist_ok=True)
    os.makedirs('data/processed', exist_ok=True)
    os.makedirs('data/features', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('results', exist_ok=True)
    
    if not check_data_exists(1) and not check_data_exists(2):
        print("No data files found. Creating sample data...")
        create_sample_data()
    
    run_complete_pipeline()
